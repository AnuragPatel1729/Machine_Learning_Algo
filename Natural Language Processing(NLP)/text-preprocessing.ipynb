{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f6cb036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1efe5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7675bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f9fe5",
   "metadata": {},
   "source": [
    "# Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a349fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff56baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. <br /><br />the...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34b3ca",
   "metadata": {},
   "source": [
    "# Remove HTML Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bbdc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e358eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(text):\n",
    "    expression = re.compile('<.*?>')\n",
    "    return expression.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c474126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdd3e742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wonderful little production. the filming technique is very unassuming- very old-time-bbc fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. the actors are extremely well chosen- michael sheen not only \"has got all the polari\" but he has all the voices down pat too! you can truly see the seamless editing guided by the references to williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. a masterful production about one of the great master\\'s of comedy and his life. the realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. it plays on our knowledge and our senses, particularly with the scenes concerning orton and halliwell and the sets (particularly of their flat with halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c5d5a6",
   "metadata": {},
   "source": [
    "# Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0378ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    exp = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return exp.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "304a4260",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'Check out new Chat GPT here https://chat.openai.com/'\n",
    "text2 = 'Check out facebook real http://www.facebook.com/reel/'\n",
    "text3 = 'Google search here www.google.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89f103e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check out new Chat GPT here '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_urls(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdbb7f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check out facebook real '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_urls(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735711fc",
   "metadata": {},
   "source": [
    "# Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ac845c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f7621a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b431c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_word = string.punctuation\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('','', char_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47132ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string With Punctuation'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'string. With. Punctuation?'\n",
    "remove_punc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b369d3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b3b9cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! \" # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ ` { | } ~ "
     ]
    }
   ],
   "source": [
    "for key in str.maketrans('','', char_word).keys():\n",
    "    print(chr(key), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c98e42",
   "metadata": {},
   "source": [
    "# Chat word treatement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5001271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_word = {\n",
    "    'AFAIK': 'As Far As I Know',\n",
    "    'AFK': 'Away From Keyboard',\n",
    "    'ASAP': 'As Soon As Possible',\n",
    "    'ATK': 'At The Keyboard',\n",
    "    'ATM': 'At The Moment',\n",
    "    'A3': 'Anytime, Anywhere, Anyplace',\n",
    "    'BAK': 'Back At Keyboard',\n",
    "    'BBL': 'Be Back Later',\n",
    "    'BBS': 'Be Back Soon',\n",
    "    'BFN': 'Bye For Now',\n",
    "    'B4N': 'Bye For Now',\n",
    "    'BRB': 'Be Right Back',\n",
    "    'BRT': 'Be Right There',\n",
    "    'BTW': 'By The Way',\n",
    "    'B4': 'Before',\n",
    "    'CU': 'See You',\n",
    "    'CUL8R': 'See You Later',\n",
    "    'CYA': 'See You',\n",
    "    'FAQ': 'Frequently Asked Questions',\n",
    "    'FC': 'Fingers Crossed',\n",
    "    'FWIW': 'For What It\\'s Worth',\n",
    "    'FYI': 'For Your Information',\n",
    "    'GAL': 'Get A Life',\n",
    "    'GG': 'Good Game',\n",
    "    'GN': 'Good Night',\n",
    "    'GMTA': 'Great Minds Think Alike',\n",
    "    'GR8': 'Great!',\n",
    "    'G9': 'Genius',\n",
    "    'IC': 'I See',\n",
    "    'ICQ': 'I Seek you (also a chat program)',\n",
    "    'ILU': 'I Love You',\n",
    "    'IMHO': 'In My Honest/Humble Opinion',\n",
    "    'IMO': 'In My Opinion',\n",
    "    'IOW': 'In Other Words',\n",
    "    'IRL': 'In Real Life',\n",
    "    'KISS': 'Keep It Simple, Stupid',\n",
    "    'LDR': 'Long Distance Relationship',\n",
    "    'LMAO': 'Laugh My A.. Off',\n",
    "    'LOL': 'Laughing Out Loud',\n",
    "    'LTNS': 'Long Time No See',\n",
    "    'L8R': 'Later',\n",
    "    'MTE': 'My Thoughts Exactly',\n",
    "    'M8': 'Mate',\n",
    "    'NRN': 'No Reply Necessary',\n",
    "    'OIC': 'Oh I See',\n",
    "    'PITA': 'Pain In The A..',\n",
    "    'PRT': 'Party',\n",
    "    'PRW': 'Parents Are Watching',\n",
    "    'QPSA': 'Que Pasa?',\n",
    "    'ROFL': 'Rolling On The Floor Laughing',\n",
    "    'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
    "    'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off',\n",
    "    'SK8': 'Skate',\n",
    "    'STATS': 'Your sex and age',\n",
    "    'ASL': 'Age, Sex, Location',\n",
    "    'THX': 'Thank You',\n",
    "    'TTFN': 'Ta-Ta For Now!',\n",
    "    'TTYL': 'Talk To You Later',\n",
    "    'U': 'You',\n",
    "    'U2': 'You Too',\n",
    "    'U4E': 'Yours For Ever',\n",
    "    'WB': 'Welcome Back',\n",
    "    'WTF': 'What The F...',\n",
    "    'WTG': 'Way To Go!',\n",
    "    'WUF': 'Where Are You From?',\n",
    "    'W8': 'Wait...',\n",
    "    '7K': 'Sick:-D Laughter',\n",
    "    'TFW': 'That Feeling When',\n",
    "    'MFW': 'My Face When',\n",
    "    'MRW': 'My Reaction When',\n",
    "    'IFYP': 'I Feel Your Pain',\n",
    "    'LOL': 'Laughing Out Loud',\n",
    "    'TNTL': 'Trying Not To Laugh',\n",
    "    'JK': 'Just Kidding',\n",
    "    'IDC': 'I Donâ€™t Care',\n",
    "    'ILY': 'I Love You',\n",
    "    'IMU': 'I Miss You',\n",
    "    'ADIH': 'Another Day In Hell',\n",
    "    'IDC': 'I Donâ€™t Care',\n",
    "    'ZZZ': 'Sleeping, Bored, Tired',\n",
    "    'WYWH': 'Wish You Were Here',\n",
    "    'TIME': 'Tears In My Eyes',\n",
    "    'BAE': 'Before Anyone Else',\n",
    "    'FIMH': 'Forever In My Heart',\n",
    "    'BSAAW': 'Big Smile And A Wink',\n",
    "    'BWL': 'Bursting With Laughter',\n",
    "    'LMAO': 'Laughing My A** Off',\n",
    "    'BFF': 'Best Friends Forever',\n",
    "    'CSL': 'Canâ€™t Stop Laughing'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1aa6e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_word:\n",
    "            new_text.append(chat_word[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4be6e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey Rishabh Be Right Back'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hey Rishabh brb\"\n",
    "chat_conversion(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a6a5f",
   "metadata": {},
   "source": [
    "# Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b5230c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\as233\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\as233\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\as233\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\as233\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\as233\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fdffeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59776006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certain conditions during several generations are modified in the same manner.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_text = \"ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.\"\n",
    "blb = TextBlob(incorrect_text)\n",
    "blb.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df00cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "419e3451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\as233\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "508ec555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "332caaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eaa4a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(sentence):\n",
    "    words = sentence.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee2114c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably all-time favorite movie, story selflessness, sacrifice dedication noble cause, preachy boring. never gets old, despite seen 15 times'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords('probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f396078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one reviewers mentioned watching 1 oz episode ...\n",
       "1        wonderful little production. filming technique...\n",
       "2        thought wonderful way spend time hot summer we...\n",
       "3        basically there's family little boy (jake) thi...\n",
       "4        petter mattei's \"love time money\" visually stu...\n",
       "                               ...                        \n",
       "49995    thought movie right good job. creative origina...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    catholic taught parochial elementary schools n...\n",
       "49998    i'm going disagree previous comment side malti...\n",
       "49999    one expects star trek movies high art, fans ex...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41470301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>i thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am a catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>no one expects the star trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      one of the other reviewers has mentioned that ...  positive\n",
       "1      a wonderful little production. the filming tec...  positive\n",
       "2      i thought this was a wonderful way to spend ti...  positive\n",
       "3      basically there's a family where a little boy ...  negative\n",
       "4      petter mattei's \"love in the time of money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  i thought this movie did a down right good job...  positive\n",
       "49996  bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  i am a catholic taught in parochial elementary...  negative\n",
       "49998  i'm going to have to disagree with the previou...  negative\n",
       "49999  no one expects the star trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc18f4",
   "metadata": {},
   "source": [
    "# Removing or Replacing Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8e538af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "                               \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "                               \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e6688b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loved the movie. It was '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emojis(\"Loved the movie. It was ðŸ˜˜ðŸ˜˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "feb35f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lmao '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emojis(\"Lmao ðŸ˜‚ðŸ˜‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93c32b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Obtaining dependency information for emoji from https://files.pythonhosted.org/packages/8d/97/fbe537350214b0489e6c7052b9e8928a85ed5febb621a82cc5437dbf17e7/emoji-2.10.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading emoji-2.10.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Downloading emoji-2.10.0-py2.py3-none-any.whl (457 kB)\n",
      "   ---------------------------------------- 0.0/457.9 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/457.9 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 30.7/457.9 kB 325.1 kB/s eta 0:00:02\n",
      "   ----- --------------------------------- 61.4/457.9 kB 544.7 kB/s eta 0:00:01\n",
      "   ------- ------------------------------- 92.2/457.9 kB 655.4 kB/s eta 0:00:01\n",
      "   ------------------- -------------------- 225.3/457.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 337.9/457.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 457.9/457.9 kB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.10.0\n"
     ]
    }
   ],
   "source": [
    "# replace\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3182208c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python is :fire:'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "emoji.demojize('Python is ðŸ”¥')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "336e275a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was :face_blowing_a_kiss:\n"
     ]
    }
   ],
   "source": [
    "print(emoji.demojize('Loved the movie. It was ðŸ˜˜'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc9f3e",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "301c1ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'name', 'is', 'Anurag', 'Singh']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spliting in words\n",
    "sent1 = \"My name is Anurag Singh\"\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d09bd9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is Anurag Singh',\n",
       " ' I am a Machine Learning Engineer',\n",
       " ' Currently working on a neural network']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting in sentence\n",
    "sent2 = \"My name is Anurag Singh. I am a Machine Learning Engineer. Currently working on a neural network\"\n",
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "486643d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm\", 'going', 'to', 'delhi!']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem with split\n",
    "sent3 = \"I'm going to delhi!\"\n",
    "sent3.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55ce3cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where do think I should go? I have 3 day holiday']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent4 = 'Where do think I should go? I have 3 day holiday'\n",
    "sent4.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef1accb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm\", 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regular expression\n",
    "sent5 = \"I'm going to delhi!\"\n",
    "tokens = re.findall(\"['\\w']+\", sent5)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14876592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry',\n",
       " \"\\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sentences = re.compile('[.!?] ').split(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "473dbfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "86452afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f71c8ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'visit', 'delhi', '!']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = 'I am going to visit delhi!'\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4ea3edd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "56318682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = 'A 5km ride cost $10.50'\n",
    "\n",
    "word_tokenize(sent5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84f01de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " \"'re\",\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " '!',\n",
       " 'mail',\n",
       " 'us',\n",
       " 'at',\n",
       " 'nks',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f98d5ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '5km', 'ride', 'cost', '$', '10.50']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a752449b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/90/f0/0133b684e18932c7bf4075d94819746cee2c0329f2569db526b0fa1df1df/spacy-3.7.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading spacy-3.7.2-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Obtaining dependency information for spacy-loggers<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl.metadata\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/71/46/af01a20ec368bd9cb49a1d2df15e3eca113bbf6952cc1f2a47f1c6801a7f/murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/c1/c3/dd044e6f62a3d317c461f6f0c153c6573ed13025752d779e514000c15dd2/cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/e4/fc/78cdbdb79f5d6d45949e72c32445d6c060977ad50a1dcfc0392622165f7c/preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.1.8 (from spacy)\n",
      "  Obtaining dependency information for thinc<8.3.0,>=8.1.8 from https://files.pythonhosted.org/packages/74/24/564a7df5b1fac0520f6b55137deea2cc0b6f7d6e66228f1645dbfd59bb33/thinc-8.2.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading thinc-8.2.2-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/8f/69/26cbf0bad11703241cb84d5324d868097f7a8faf2f1888354dac8883f3fc/wasabi-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/eb/f5/e3f29993f673d91623df6413ba64e815dd2676fd7932cbc5e7347402ddae/srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Obtaining dependency information for weasel<0.4.0,>=0.1.0 from https://files.pythonhosted.org/packages/d5/e5/b63b8e255d89ba4155972990d42523251d4d1368c4906c646597f63870e2/weasel-0.3.4-py3-none-any.whl.metadata\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.9/45.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     ---------------------------------------- 0.0/181.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 181.6/181.6 kB 5.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/2f/09/da0592c74560cc33396504698122f7a56747c82a5e072ca7d2c3397898e1/blis-0.7.11-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading blis-0.7.11-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/39/78/f9d18da7b979a2e6007bfcea2f3c8cc02ed210538ae1ce7e69092aed7b18/confection-0.1.4-py3-none-any.whl.metadata\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\as233\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Obtaining dependency information for cloudpathlib<0.17.0,>=0.7.0 from https://files.pythonhosted.org/packages/0f/6e/45b57a7d4573d85d0b0a39d99673dc1f5eea9d92a1a4603b35e968fbf89a/cloudpathlib-0.16.0-py3-none-any.whl.metadata\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Downloading spacy-3.7.2-cp311-cp311-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/12.1 MB 6.5 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.6/12.1 MB 6.5 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.9/12.1 MB 6.6 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.3/12.1 MB 6.6 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.5/12.1 MB 6.8 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.8/12.1 MB 6.3 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.1/12.1 MB 6.3 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.3/12.1 MB 6.4 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.1 MB 6.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.6/12.1 MB 5.5 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.6/12.1 MB 5.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.7/12.1 MB 5.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.9/12.1 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.0/12.1 MB 4.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.1/12.1 MB 4.5 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.3/12.1 MB 4.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.6/12.1 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.8/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.9/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.1/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.4/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.5/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.7/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.8/12.1 MB 4.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.0/12.1 MB 4.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.5/12.1 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.7/12.1 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.9/12.1 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.0/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.0/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.2/12.1 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.3/12.1 MB 4.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.3/12.1 MB 4.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.4/12.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.6/12.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.6/12.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.8/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.0/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.3/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.5/12.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.7/12.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.7/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 8.0/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 8.1/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.2/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.4/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.6/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.6/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.8/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.9/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.1/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.3/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.5/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.5/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.5/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.5/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.5/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.5/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.5/12.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.1 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.7/12.1 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.0/12.1 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.1/12.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.3/12.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.4/12.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.6/12.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.7/12.1 MB 3.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.9/12.1 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.1/12.1 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.2/12.1 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.4/12.1 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.6/12.1 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.7/12.1 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.9/12.1 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 3.0 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.3/122.3 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
      "   ---------------------------------------- 0.0/479.7 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 204.8/479.7 kB 6.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 327.7/479.7 kB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  471.0/479.7 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 479.7/479.7 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.2-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.2/1.5 MB 4.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.4/1.5 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.9 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB ? eta 0:00:00\n",
      "Downloading blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.6 MB 5.9 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.6/6.6 MB 6.0 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.8/6.6 MB 5.6 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.1/6.6 MB 5.8 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.3/6.6 MB 5.6 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.5/6.6 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.7/6.6 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.0/6.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.2/6.6 MB 5.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.5/6.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.8/6.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.0/6.6 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.3/6.6 MB 5.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.6/6.6 MB 5.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.9/6.6 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.1/6.6 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.3/6.6 MB 5.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.6/6.6 MB 5.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.9/6.6 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.3/6.6 MB 5.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.6/6.6 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.8/6.6 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.1/6.6 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.4/6.6 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 5.3 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.0/45.0 kB ? eta 0:00:00\n",
      "Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, murmurhash, langcodes, cloudpathlib, catalogue, blis, typer, srsly, preshed, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.2 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a9e5c93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "373b223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9111ab9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 330.3 kB/s eta 0:00:39\n",
      "     --------------------------------------- 0.1/12.8 MB 525.1 kB/s eta 0:00:25\n",
      "      --------------------------------------- 0.3/12.8 MB 1.7 MB/s eta 0:00:08\n",
      "     - -------------------------------------- 0.5/12.8 MB 2.7 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 0.7/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.9/12.8 MB 3.5 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.1/12.8 MB 3.7 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 3.9 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.6/12.8 MB 4.1 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 4.1 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.0/12.8 MB 4.3 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.3/12.8 MB 4.3 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.5/12.8 MB 4.4 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.8/12.8 MB 4.5 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.0/12.8 MB 4.4 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.2/12.8 MB 4.5 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.5/12.8 MB 4.5 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 4.5 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 3.9/12.8 MB 4.4 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 4.1/12.8 MB 4.6 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.4/12.8 MB 4.6 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.6/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.7/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.0/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.3/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.4/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.9/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 6.0/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.2/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.5/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.7/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.9/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.8/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.0/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.2/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.5/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.7/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.3/12.8 MB 4.3 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.8/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.2/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.4/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.6/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.9/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.1/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.4/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.2/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.4/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\as233\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\as233\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c16ee723",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(sent5)\n",
    "doc2 = nlp(sent6)\n",
    "doc3 = nlp(sent7)\n",
    "doc4 = nlp(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee7bd7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "ride\n",
      "cost\n",
      "$\n",
      "10.50\n"
     ]
    }
   ],
   "source": [
    "for token in doc3:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ae4477",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bfc72af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "754ed6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def  stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2fff7fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"walk walks walking walked\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "27be0921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie\n"
     ]
    }
   ],
   "source": [
    "text = 'probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "422900c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl my alltim favorit movi a stori of selfless sacrific and dedic to a nobl caus but it not preachi or bore it just never get old despit my have seen it some 15 or more time in the last 25 year paul luka perform bring tear to my eye and bett davi in one of her veri few truli sympathet role is a delight the kid are as grandma say more like dressedup midget than children but that onli make them more fun to watch and the mother slow awaken to what happen in the world and under her own roof is believ and startl if i had a dozen thumb theyd all be up for thi movi'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ca2d131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               Noun                Verb                Adjective           Adverb              \n",
      "He                  He                  He                  He                  He                  He                  \n",
      "was                 wa                  be                  be                  was                 was                 \n",
      "running             running             run                 run                 running             running             \n",
      "and                 and                 and                 and                 and                 and                 \n",
      "eating              eating              eat                 eat                 eating              eating              \n",
      "at                  at                  at                  at                  at                  at                  \n",
      "same                same                same                same                same                same                \n",
      "time                time                time                time                time                time                \n",
      "He                  He                  He                  He                  He                  He                  \n",
      "has                 ha                  have                have                has                 has                 \n",
      "bad                 bad                 bad                 bad                 bad                 bad                 \n",
      "habit               habit               habit               habit               habit               habit               \n",
      "of                  of                  of                  of                  of                  of                  \n",
      "swimming            swimming            swim                swim                swimming            swimming            \n",
      "after               after               after               after               after               after               \n",
      "playing             playing             play                play                playing             playing             \n",
      "long                long                long                long                long                long                \n",
      "hours               hour                hours               hours               hours               hours               \n",
      "in                  in                  in                  in                  in                  in                  \n",
      "the                 the                 the                 the                 the                 the                 \n",
      "Sun                 Sun                 Sun                 Sun                 Sun                 Sun                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\as233\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "sentence_words = [word for word in sentence_words if word not in punctuations]\n",
    "# for word in sentence_words:\n",
    "#     if word in punctuations:\n",
    "#         sentence_words.remove(word)\n",
    "# sentence_words\n",
    "print(\"{0:20}{1:20}{2:20}{3:20}{4:20}{5:20}\".format(\"Word\", \"Lemma\", \"Noun\", \"Verb\", \"Adjective\", \"Adverb\"))\n",
    "for word in sentence_words:\n",
    "    print(\"{0:20}{1:20}{3:20}{3:20}{4:20}{5:20}\".format(word, wordnet_lemmatizer.lemmatize(word),\n",
    "                                                        wordnet_lemmatizer.lemmatize(word, pos='n'),\n",
    "                                                        wordnet_lemmatizer.lemmatize(word, pos='v'),\n",
    "                                                       wordnet_lemmatizer.lemmatize(word, pos='a'),\n",
    "                                                       wordnet_lemmatizer.lemmatize(word, pos='r'),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31344ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
